{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, auc, RocCurveDisplay, roc_curve, average_precision_score, precision_recall_curve\n",
    "import json\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder = os.path.join(\"/\", \"Data\", \"CREMEv2_Result\", \"20230310\", \"logs_working\", \"toTrain\")\n",
    "train_technique = [\"label_accounting_train_technique.csv\",\n",
    "                   \"relabel_syslog_train_technique.csv\",\n",
    "                   \"label_traffic_train_technique.csv\"]\n",
    "train_lifecycle = [\"label_accounting_train_lifecycle.csv\",\n",
    "                   \"relabel_syslog_train_lifecycle.csv\",\n",
    "                   \"label_traffic_train_lifecycle.csv\"]\n",
    "datas = [\"accounting\", \"syslog\", \"traffic\"]\n",
    "\n",
    "model_folder = os.path.join(\"/\", \"Data\", \"CREMEv2_Result\", \"20230310\", \"logs_working\", \"model\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for data in train_technique:\n",
    "    if os.path.exists(os.path.join(folder, data)):\n",
    "        print(\"Path is exists: \", data)\n",
    "    else:\n",
    "        print(\"Path is not exists: \", data)\n",
    "\n",
    "for data in train_lifecycle:\n",
    "    if os.path.exists(os.path.join(folder, data)):\n",
    "        print(\"Path is exists: \", data)\n",
    "    else:\n",
    "        print(\"Path is not exists: \", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Model Definition,Parameters Settings, and Evaluation Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_state = 42\n",
    "core = -1\n",
    "# model = XGBClassifier(objective='multi:softprob', eval_metric='merror', n_jobs=core)\n",
    "\n",
    "\n",
    "\n",
    "models = {}\n",
    "# model_name_technique = []\n",
    "# model_name_lifecycle = []\n",
    "\n",
    "### Linear-based\n",
    "models['Logistic_Regresion'] = LogisticRegression(max_iter=1500, n_jobs=core, verbose=True)\n",
    "\n",
    "### Tree-based\n",
    "models['Decision_Tree'] = DecisionTreeClassifier()\n",
    "\n",
    "# ### SVM-based\n",
    "# models['SVM'] = SVC(kernel='linear', gamma='auto', verbose=True)\n",
    "\n",
    "### Naive bayes\n",
    "models['Naive_Bayes'] = GaussianNB()\n",
    "\n",
    "### KNN-based\n",
    "models['KNN'] = KNeighborsClassifier(n_jobs=core)\n",
    "\n",
    "### ensemble-based\n",
    "models['XGBoost'] = XGBClassifier(objective='multi:softprob', eval_metric='merror', n_jobs=core, verbosity=2)\n",
    "\n",
    "\n",
    "\n",
    "evaluation_technique = {}\n",
    "evaluation_lifecycle = {}\n",
    "\n",
    "\n",
    "evaluation_roc_technique = {}\n",
    "evaluation_roc_lifecycle = {}\n",
    "\n",
    "evaluation_prauc_technique = {}\n",
    "evaluation_prauc_lifecycle = {}\n",
    "\n",
    "## accuracy, precision, recall, and F1-score\n",
    "for data_type in datas:\n",
    "    evaluation_technique[data_type] = {}\n",
    "    for name in models:\n",
    "        evaluation_technique[data_type][name] = {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1_score': []\n",
    "        }\n",
    "\n",
    "for data_type in datas:\n",
    "    evaluation_lifecycle[data_type] = {}\n",
    "    for name in models:\n",
    "        evaluation_lifecycle[data_type][name] = {\n",
    "            'accuracy': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1_score': []\n",
    "        }\n",
    "\n",
    "# ROC-AUC\n",
    "for data_type in datas:\n",
    "    evaluation_roc_technique[data_type] = {}\n",
    "    for name in models:\n",
    "        evaluation_roc_technique[data_type][name] = {\n",
    "            'roc': []\n",
    "        }\n",
    "\n",
    "for data_type in datas:\n",
    "    evaluation_roc_lifecycle[data_type] = {}\n",
    "    for name in models:\n",
    "        evaluation_roc_lifecycle[data_type][name] = {\n",
    "            'roc': []\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "## PR-AUC\n",
    "for data_type in datas:\n",
    "    evaluation_prauc_technique[data_type] = {}\n",
    "    for name in models:\n",
    "        evaluation_prauc_technique[data_type][name] = {\n",
    "            'precision': {},\n",
    "            'recall': {},\n",
    "            'average_precision': {}\n",
    "        }\n",
    "\n",
    "for data_type in datas:\n",
    "    evaluation_prauc_lifecycle[data_type] = {}\n",
    "    for name in models:\n",
    "        evaluation_prauc_lifecycle[data_type][name] = {\n",
    "            'precision': {},\n",
    "            'recall': {},\n",
    "            'average_precision': {}\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(evaluation_technique)\n",
    "print(\"=========================================================================================\")\n",
    "print(evaluation_lifecycle)\n",
    "print(\"=========================================================================================\")\n",
    "print(\"=========================================================================================\")\n",
    "print(evaluation_roc_technique)\n",
    "print(\"=========================================================================================\")\n",
    "print(evaluation_roc_lifecycle)\n",
    "print(\"=========================================================================================\")\n",
    "print(\"=========================================================================================\")\n",
    "print(evaluation_prauc_technique)\n",
    "print(\"=========================================================================================\")\n",
    "print(evaluation_prauc_lifecycle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Training and evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Training and evaluating (Technique)**\n",
    "\n",
    "\n",
    "* Training\n",
    "* Evaluation\n",
    "  * Precision\n",
    "  * Recall\n",
    "  * F1-score\n",
    "  * ROC-AUC\n",
    "  * PR-AUC\n",
    "* Data Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for data in train_technique:\n",
    "    for data_type in datas:\n",
    "\n",
    "        remove_extension = data.split('.')\n",
    "        name_without_ext = remove_extension[0].split('_')\n",
    "\n",
    "        if data_type == name_without_ext[1]:\n",
    "            print(f\"Processing dataset {i}: {data}\")\n",
    "\n",
    "            df = pd.read_csv(os.path.join(folder, data))\n",
    "\n",
    "\n",
    "            label_origin = sorted([int(i) for i in df['Label'].unique()])\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(df['Label'])\n",
    "            le_origin_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            origin_le_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "\n",
    "            X = df.drop(columns=['Label'])\n",
    "            X = X.to_numpy()\n",
    "            # X = X.reshape(-1)\n",
    "            y = df['Label']\n",
    "            y = y.to_numpy()\n",
    "            y = y.reshape(-1)\n",
    "            y = le.transform(y)\n",
    "\n",
    "            class_label = list(label_origin)\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Train Test Split {data}\")\n",
    "            X_train_technique, X_test_technique, y_train_technique, y_test_technique = train_test_split(X, y, test_size = 0.2, random_state=r_state)\n",
    "\n",
    "            print(f\"Data balancing of {data}\")\n",
    "            X_train_technique, y_train_technique = SMOTE(n_jobs=-1, random_state=r_state).fit_resample(X_train_technique, y_train_technique)\n",
    "            j = 1\n",
    "            for name, model in models.items():\n",
    "                model_filename = '{}{}_model_{}_{}_{}.pkl'.format(i, j, name, data_type, name_without_ext[-1])\n",
    "                print(f\"{i}{j}. Model {name} --> filename = {model_filename}\")\n",
    "                print(\"===================================================\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                if os.path.exists(os.path.join(model_folder, model_filename)):\n",
    "                    print(\"Load \", model_filename)\n",
    "                    model = pickle.load(open(os.path.join(model_folder, model_filename), 'rb'))\n",
    "                else:\n",
    "                    print(f\"training model {name} on {data}\")\n",
    "                    model.fit(X_train_technique, y_train_technique)\n",
    "                    print(\"Dump \", model_filename)\n",
    "                    pickle.dump(model, open(os.path.join(model_folder, model_filename), 'wb'))\n",
    "                y_pred_technique = model.predict(X_test_technique)\n",
    "                y_score_technique = model.predict_proba(X_test_technique)\n",
    "                label_binarizer_technique = LabelBinarizer().fit(y_train_technique)\n",
    "                y_onehot_test_technique = label_binarizer_technique.transform(y_test_technique)\n",
    "\n",
    "                evaluation_technique[data_type][name]['accuracy'].append(accuracy_score(y_test_technique, y_pred_technique))\n",
    "                evaluation_technique[data_type][name]['precision'].append(precision_score(y_test_technique, y_pred_technique, average='weighted',zero_division=0))\n",
    "                evaluation_technique[data_type][name]['recall'].append(recall_score(y_test_technique, y_pred_technique, average='weighted', zero_division=0))\n",
    "                evaluation_technique[data_type][name]['f1_score'].append(f1_score(y_test_technique, y_pred_technique, average='weighted', zero_division=0))\n",
    "                evaluation_roc_technique[data_type][name]['roc'] = list((y_test_technique, y_onehot_test_technique, y_score_technique))\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Execution Time: {:.2f}\\n\".format(end_time - start_time))\n",
    "                j += 1\n",
    "            else:\n",
    "                continue\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "\n",
    "\n",
    "i=1\n",
    "for data in train_technique:\n",
    "    for data_type in datas:\n",
    "\n",
    "        remove_extension = data.split('.')\n",
    "        name_without_ext = remove_extension[0].split('_')\n",
    "\n",
    "        if data_type == name_without_ext[1]:\n",
    "            print(f\"Processing dataset {i}: {data}\")\n",
    "\n",
    "            df = pd.read_csv(os.path.join(folder, data))\n",
    "\n",
    "\n",
    "            label_origin = sorted([int(i) for i in df['Label'].unique()])\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(df['Label'])\n",
    "            le_origin_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            origin_le_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "\n",
    "            X = df.drop(columns=['Label'])\n",
    "            X = X.to_numpy()\n",
    "            # X = X.reshape(-1)\n",
    "            y = df['Label']\n",
    "            y = y.to_numpy()\n",
    "            y = y.reshape(-1)\n",
    "            y = le.transform(y)\n",
    "\n",
    "            class_label = list(label_origin)\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Train Test Split {data}\")\n",
    "            X_train_technique_prauc, X_test_technique_prauc, y_train_technique_prauc, y_test_technique_prauc = train_test_split(X, y, test_size = 0.2, random_state=r_state)\n",
    "\n",
    "            print(f\"Data balancing of {data}\")\n",
    "            X_train_technique_prauc, y_train_technique_prauc = SMOTE(n_jobs=-1, random_state=r_state).fit_resample(X_train_technique_prauc, y_train_technique_prauc)\n",
    "            j = 1\n",
    "            for name, model in models.items():\n",
    "                model_filename = '{}{}_model_prauc_{}_{}_{}.pkl'.format(i, j, name, data_type, name_without_ext[-1])\n",
    "                print(f\"{i}{j}. Model {name} --> filename = {model_filename}\")\n",
    "                print(\"===================================================\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                if os.path.exists(os.path.join(model_folder, model_filename)):\n",
    "                    print(\"Load \", model_filename)\n",
    "                    model = pickle.load(open(os.path.join(model_folder, model_filename), 'rb'))\n",
    "                else:\n",
    "                    print(f\"training model {name} on {data}\")\n",
    "                    model = OneVsRestClassifier(model).fit(X_train_technique_prauc, y_train_technique_prauc)\n",
    "                    print(\"Dump \", model_filename)\n",
    "                    pickle.dump(model, open(os.path.join(model_folder, model_filename), 'wb'))\n",
    "                y_score_technique_prauc = model.decision_function(X_test_technique_prauc)\n",
    "                label_binarizer_technique_prauc = LabelBinarizer().fit(y_train_technique_prauc)\n",
    "                y_onehot_test_technique_prauc = label_binarizer_technique_prauc.transform(y_test_technique_prauc)\n",
    "\n",
    "                for k in range(y):\n",
    "                    precision[k], recall[k] = precision_recall_curve(y_test_technique_prauc[:, k], y_score_technique_prauc[:, k])\n",
    "                    average_precision[k] = average_precision_score(y_test_technique_prauc[:, k], y_score_technique_prauc[:, k])\n",
    "\n",
    "                evaluation_prauc_technique[data_type][name]['precision'].append(precision)\n",
    "                evaluation_prauc_technique[data_type][name]['recall'].append(recall)\n",
    "                evaluation_prauc_technique[data_type][name]['average_precision'].append(average_precision)\n",
    "\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Execution Time: {:.2f}\\n\".format(end_time - start_time))\n",
    "                j += 1\n",
    "            else:\n",
    "                continue\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluation_technique)\n",
    "print(\"=======================================================================================================\")\n",
    "print(evaluation_roc_technique)\n",
    "print(\"=======================================================================================================\")\n",
    "print(evaluation_prauc_technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"evaluation_result_technique.json\"\n",
    "with open(os.path.join(model_folder, json_filename), 'w') as json_file:\n",
    "    json.dump(evaluation_technique, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"evaluation_result_roc_technique.json\"\n",
    "with open(os.path.join(model_folder, json_filename), 'w') as json_file:\n",
    "    json.dump(evaluation_roc_technique, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"evaluation_result_prauc_technique.json\"\n",
    "with open(os.path.join(model_folder, json_filename), 'w') as json_file:\n",
    "    json.dump(evaluation_prauc_technique, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Training and evaluating (Lifecycle)**\n",
    "* Training\n",
    "* Evaluation\n",
    "  * Precision\n",
    "  * Recall\n",
    "  * F1-score\n",
    "  * ROC-AUC\n",
    "  * PR-AUC\n",
    "* Data Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "for data in train_lifecycle:\n",
    "    for data_type in datas:\n",
    "\n",
    "        remove_extension = data.split('.')\n",
    "        name_without_ext = remove_extension[0].split('_')\n",
    "\n",
    "        if data_type == name_without_ext[1]:\n",
    "            print(f\"Processing dataset {i}: {data}\")\n",
    "\n",
    "            df = pd.read_csv(os.path.join(folder, data))\n",
    "\n",
    "\n",
    "            label_origin = sorted([int(i) for i in df['Label_lifecycle'].unique()])\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(df['Label_lifecycle'])\n",
    "            le_origin_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            origin_le_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "\n",
    "            X = df.drop(columns=['Label_lifecycle'])\n",
    "            X = X.to_numpy()\n",
    "            # X = X.reshape(-1)\n",
    "            y = df['Label_lifecycle']\n",
    "            y = y.to_numpy()\n",
    "            y = y.reshape(-1)\n",
    "            y = le.transform(y)\n",
    "\n",
    "            class_label = list(label_origin)\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Train Test Split {data}\")\n",
    "            X_train_lifecycle, X_test_lifecycle, y_train_lifecycle, y_test_lifecycle = train_test_split(X, y, test_size = 0.2, random_state=r_state)\n",
    "\n",
    "            print(f\"Data balancing of {data}\")\n",
    "            X_train_lifecycle, y_train_lifecycle = SMOTE(n_jobs=-1, random_state=r_state).fit_resample(X_train_lifecycle, y_train_lifecycle)\n",
    "            j = 1\n",
    "            for name, model in models.items():\n",
    "                model_filename = '{}{}_model_{}_{}_{}.pkl'.format(i, j, name, data_type, name_without_ext[-1])\n",
    "                print(f\"{i}{j}. Model {name} --> filename = {model_filename}\")\n",
    "                print(\"===================================================\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                if os.path.exists(os.path.join(model_folder, model_filename)):\n",
    "                    print(\"Load \", model_filename)\n",
    "                    model = pickle.load(open(os.path.join(model_folder, model_filename), 'rb'))\n",
    "                else:\n",
    "                    print(f\"training model {name} on {data}\")\n",
    "                    model.fit(X_train_lifecycle, y_train_lifecycle)\n",
    "                    print(\"Dump \", model_filename)\n",
    "                    pickle.dump(model, open(os.path.join(model_folder, model_filename), 'wb'))\n",
    "                y_pred_lifecycle = model.predict(X_test_lifecycle)\n",
    "                y_score_lifecycle = model.predict_proba(X_test_lifecycle)\n",
    "                label_binarizer_lifecycle = LabelBinarizer().fit(y_train_lifecycle)\n",
    "                y_onehot_test_lifecycle = label_binarizer_lifecycle.transform(y_test_lifecycle)\n",
    "\n",
    "                evaluation_lifecycle[data_type][name]['accuracy'].append(accuracy_score(y_test_lifecycle, y_pred_lifecycle))\n",
    "                evaluation_lifecycle[data_type][name]['precision'].append(precision_score(y_test_lifecycle, y_pred_lifecycle, average='weighted',zero_division=0))\n",
    "                evaluation_lifecycle[data_type][name]['recall'].append(recall_score(y_test_lifecycle, y_pred_lifecycle, average='weighted', zero_division=0))\n",
    "                evaluation_lifecycle[data_type][name]['f1_score'].append(f1_score(y_test_lifecycle, y_pred_lifecycle, average='weighted', zero_division=0))\n",
    "                evaluation_roc_lifecycle[data_type][name]['roc'] = list((y_test_lifecycle, y_onehot_test_lifecycle, y_score_lifecycle))\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Execution Time: {:.2f}\\n\".format(end_time - start_time))\n",
    "                j += 1\n",
    "            else:\n",
    "                continue\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "\n",
    "\n",
    "i=1\n",
    "for data in train_lifecycle:\n",
    "    for data_type in datas:\n",
    "\n",
    "        remove_extension = data.split('.')\n",
    "        name_without_ext = remove_extension[0].split('_')\n",
    "\n",
    "        if data_type == name_without_ext[1]:\n",
    "            print(f\"Processing dataset {i}: {data}\")\n",
    "\n",
    "            df = pd.read_csv(os.path.join(folder, data))\n",
    "\n",
    "\n",
    "            label_origin = sorted([int(i) for i in df['Label_lifecycle'].unique()])\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(df['Label_lifecycle'])\n",
    "            le_origin_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "            origin_le_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "\n",
    "            X = df.drop(columns=['Label_lifecycle'])\n",
    "            X = X.to_numpy()\n",
    "            # X = X.reshape(-1)\n",
    "            y = df['Label_lifecycle']\n",
    "            y = y.to_numpy()\n",
    "            y = y.reshape(-1)\n",
    "            y = le.transform(y)\n",
    "\n",
    "            class_label = list(label_origin)\n",
    "\n",
    "\n",
    "\n",
    "            print(f\"Train Test Split {data}\")\n",
    "            X_train_lifecycle_prauc, X_test_lifecycle_prauc, y_train_lifecycle_prauc, y_test_lifecycle_prauc = train_test_split(X, y, test_size = 0.2, random_state=r_state)\n",
    "\n",
    "            print(f\"Data balancing of {data}\")\n",
    "            X_train_lifecycle_prauc, y_train_lifecycle_prauc = SMOTE(n_jobs=-1, random_state=r_state).fit_resample(X_train_lifecycle_prauc, y_train_lifecycle_prauc)\n",
    "            j = 1\n",
    "            for name, model in models.items():\n",
    "                model_filename = '{}{}_model_prauc_{}_{}_{}.pkl'.format(i, j, name, data_type, name_without_ext[-1])\n",
    "                print(f\"{i}{j}. Model {name} --> filename = {model_filename}\")\n",
    "                print(\"===================================================\")\n",
    "\n",
    "                start_time = time.time()\n",
    "                if os.path.exists(os.path.join(model_folder, model_filename)):\n",
    "                    print(\"Load \", model_filename)\n",
    "                    model = pickle.load(open(os.path.join(model_folder, model_filename), 'rb'))\n",
    "                else:\n",
    "                    print(f\"training model {name} on {data}\")\n",
    "                    model = OneVsRestClassifier(model).fit(X_train_lifecycle_prauc, Y_train_lifecycle_prauc)\n",
    "                    print(\"Dump \", model_filename)\n",
    "                    pickle.dump(model, open(os.path.join(model_folder, model_filename), 'wb'))\n",
    "                y_score_lifecycle_prauc = model.decision_function(X_test_lifecycle_prauc)\n",
    "                label_binarizer_lifecycle_prauc = LabelBinarizer().fit(y_train_lifecycle_prauc)\n",
    "                y_onehot_test_lifecycle_prauc = label_binarizer_lifecycle_prauc.transform(y_test_lifecycle_prauc)\n",
    "\n",
    "                for k in range(y):\n",
    "                    precision[k], recall[k] = precision_recall_curve(y_test_lifecycle_prauc[:, k], y_score_lifecycle_prauc[:, k])\n",
    "                    average_precision[k] = average_precision_score(y_test_lifecycle_prauc[:, k], y_score_lifecycle_prauc[:, k])\n",
    "\n",
    "                evaluation_prauc_lifecycle[data_type][name]['precision'].append(precision)\n",
    "                evaluation_prauc_lifecycle[data_type][name]['recall'].append(recall)\n",
    "                evaluation_prauc_lifecycle[data_type][name]['average_precision'].append(average_precision)\n",
    "\n",
    "\n",
    "                end_time = time.time()\n",
    "\n",
    "                print(\"Execution Time: {:.2f}\\n\".format(end_time - start_time))\n",
    "                j += 1\n",
    "            else:\n",
    "              continue\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"evaluation_result_lifecycle.json\"\n",
    "with open(os.path.join(model_folder, json_filename), 'w') as json_file:\n",
    "    json.dump(evaluation_lifecycle, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"evaluation_result_lifecycle.json\"\n",
    "with open(os.path.join(model_folder, json_filename), 'w') as json_file:\n",
    "    json.dump(evaluation_roc_lifecycle, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"evaluation_result_lifecycle.json\"\n",
    "with open(os.path.join(model_folder, json_filename), 'w') as json_file:\n",
    "    json.dump(evaluation_prauc_lifecycle, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
